{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def token_with_punc(text):\n",
    "    # seperate punctuations and letters by adding space before and after each punctuation\n",
    "    translate_table = dict((ord(char), ' ' + char + ' ') for char in string.punctuation)\n",
    "    # create a map to get tokens\n",
    "    text = text[0].translate(translate_table)\n",
    "    # return the tokens after splitting by whitespace\n",
    "    return text.lower().split()\n",
    "\n",
    "# remove punctuations\n",
    "def punc_rem(text):\n",
    "    # create a string of punctuations that need to be removed\n",
    "    punc_list = ',!\"#$%&()*+/:;<=>@[\\\\]^`{|}~\\t\\n'\n",
    "    # remove punctuations as required\n",
    "    text = [w for w in text if w not in punc_list]\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def stop_word_rem(text):\n",
    "    # using stopwords offered by NLTK\n",
    "    text = [ w for w in text if w not in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "# return a list with stopwords\n",
    "def clean_with_stopwords(text):\n",
    "    text = token_with_punc(text)\n",
    "    text = punc_rem(text)\n",
    "    return text\n",
    "\n",
    "# return a list without stopwords\n",
    "def clean_no_stopwords(text):\n",
    "    text = clean_with_stopwords(text)\n",
    "    text = stop_word_rem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(\"/Users/mgh/MGH/CE/Waterloo/MSCI_641_NLP/Text-Analytics/pos.txt\",header=None, sep=\"\\n\")\n",
    "df_neg = pd.read_csv(\"/Users/mgh/MGH/CE/Waterloo/MSCI_641_NLP/Text-Analytics/neg.txt\",header=None, sep=\"\\n\")\n",
    "frames = [df_pos,df_neg]\n",
    "df = pd.concat(frames).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_with_stopwords = []\n",
    "for doc in df:\n",
    "    doc = clean_with_stopwords(doc)\n",
    "    token_with_stopwords.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['she',\n",
       "  'has',\n",
       "  'already',\n",
       "  'tried',\n",
       "  'one',\n",
       "  'recipe',\n",
       "  'a',\n",
       "  'day',\n",
       "  'after',\n",
       "  'receiving',\n",
       "  'the',\n",
       "  'book',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'bought',\n",
       "  'this',\n",
       "  'zoku',\n",
       "  'quick',\n",
       "  'pop',\n",
       "  'for',\n",
       "  'my',\n",
       "  'daughterr',\n",
       "  'with',\n",
       "  'her',\n",
       "  'zoku',\n",
       "  'quick',\n",
       "  'maker',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'loves',\n",
       "  'it',\n",
       "  'and',\n",
       "  'have',\n",
       "  'fun',\n",
       "  'to',\n",
       "  'make',\n",
       "  'her',\n",
       "  'own',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'was',\n",
       "  'hoping',\n",
       "  'there',\n",
       "  'were',\n",
       "  'more',\n",
       "  'where',\n",
       "  'those',\n",
       "  'came',\n",
       "  'from',\n",
       "  '.']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_with_stopwords[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers =2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(token_with_stopwords, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146556806, 411857280)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(token_with_stopwords, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7956719398498535),\n",
       " ('nice', 0.6755967140197754),\n",
       " ('decent', 0.6724581122398376),\n",
       " ('excellent', 0.6036227941513062),\n",
       " ('fantastic', 0.5993865132331848),\n",
       " ('wonderful', 0.5628539323806763),\n",
       " ('amazing', 0.5339518189430237),\n",
       " ('terrific', 0.5239437818527222),\n",
       " ('bad', 0.5177990198135376),\n",
       " ('superb', 0.5166008472442627),\n",
       " ('fabulous', 0.4999239444732666),\n",
       " ('awesome', 0.4963359534740448),\n",
       " ('reasonable', 0.49632614850997925),\n",
       " ('well', 0.4856538474559784),\n",
       " ('impressive', 0.4832674562931061),\n",
       " ('perfect', 0.4740520715713501),\n",
       " ('outstanding', 0.46723324060440063),\n",
       " ('poor', 0.4488271474838257),\n",
       " ('exceptional', 0.4462299942970276),\n",
       " ('ok', 0.43502235412597656)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"good\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.5330793857574463),\n",
       " ('horrible', 0.5330667495727539),\n",
       " ('awful', 0.5190057754516602),\n",
       " ('good', 0.5177990198135376),\n",
       " ('funny', 0.4361669719219208),\n",
       " ('poor', 0.4178902804851532),\n",
       " ('weird', 0.4173131585121155),\n",
       " ('nasty', 0.4119377136230469),\n",
       " ('strange', 0.40533795952796936),\n",
       " ('gross', 0.39210638403892517),\n",
       " ('shabby', 0.3865605592727661),\n",
       " ('ok', 0.38646817207336426),\n",
       " ('scary', 0.3818008303642273),\n",
       " ('okay', 0.37603986263275146),\n",
       " ('hideous', 0.37410056591033936),\n",
       " ('horrid', 0.3651323914527893),\n",
       " ('disappointing', 0.35796576738357544),\n",
       " ('stupid', 0.35405805706977844),\n",
       " ('worse', 0.3531869053840637),\n",
       " ('wierd', 0.3439233899116516)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bad\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
